# -*- coding: utf-8 -*-
"""RAG_5/13

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MglFRg_ZLky4dcvIRSzWtjcoCfulkMFb

# Enviroment
"""

# ========================================
# ç³»ç»Ÿçº§ä¾èµ–å®‰è£… (éœ€rootæƒé™)
# ========================================
!apt-get install antiword -qq

# ========================================
# æ ¸å¿ƒPythonåŒ…å®‰è£…
# ========================================
!pip install pymupdf nltk chromadb sentence-transformers gradio -q

# ========================================
# NLPå·¥å…·åˆå§‹åŒ–
# ========================================
import nltk
nltk.download('punkt_tab')

# ========================================
# å¯é€‰ç»„ä»¶å®‰è£…
# ========================================
# å¦‚éœ€Elasticsearchæ”¯æŒå¯å–æ¶ˆæ³¨é‡Š
# !pip install elasticsearch

# ========================================
# ä¸­æ–‡å¤„ç†æ‰©å±•
# ========================================
!pip install rank_bm25 jieba

# ========================================
# æ ‡å‡†åº“å¯¼å…¥
# ========================================
import os
import re
import sys
import time
import datetime
import hashlib
from pathlib import Path
from threading import Thread, Lock
from typing import List, Dict, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed

# ========================================
# æ–‡ä»¶/æ–‡æ¡£å¤„ç†
# ========================================
import fitz  # PyMuPDF

# ========================================
# NLP/MLç›¸å…³åº“
# ========================================
import nltk
from nltk.tokenize import sent_tokenize
import jieba
import numpy as np
import torch
from torch import cuda
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer, CrossEncoder

# ========================================
# å‘é‡æ•°æ®åº“/å­˜å‚¨
# ========================================
import chromadb
from chromadb.utils import embedding_functions

# ========================================
# æ¥å£/å¯è§†åŒ–
# ========================================
import gradio as gr
from tqdm import tqdm

# ========================================
# ç¬¬ä¸‰æ–¹æœåŠ¡/API
# ========================================
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential

# ========================================
# è°ƒè¯•/å·¥å…·
# ========================================
import traceback

# ========================================
# å·²æ³¨é‡Šçš„å¤‡ç”¨ä¾èµ–
# ========================================
# from elasticsearch import Elasticsearch

from google.colab import drive
drive.mount('/content/drive')

"""# Config"""

# =============================
# é…ç½®å‚æ•°ï¼ˆç»Ÿä¸€ç®¡ç†ï¼‰
# =============================
class Config:
    # å¤šçº¿ç¨‹é…ç½®
    NUM_WORKERS = 20
    USE_MULTITHREAD = True

    # è·¯å¾„é…ç½®
    UPLOAD_DIR = "/content/drive/MyDrive/513"
    DB_PATH = "/content/drive/MyDrive/513/chroma_db"

    # æ–‡ä»¶é…ç½®
    SUPPORTED_EXTS = [".pdf", ".PDF"]  # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
    MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MBæ–‡ä»¶å¤§å°é™åˆ¶

    # å¤„ç†å‚æ•°
    CHUNK_SIZE = 300
    OVERLAP = 50
    TOP_K = 100        # åˆå§‹æ£€ç´¢æ•°é‡
    TOP_K_FINAL = 5   # æœ€ç»ˆè¿”å›æ•°é‡
    BATCH_SIZE = 16

    # æ¨¡å‹é…ç½®
    EMBEDDING_MODEL = "BAAI/bge-m3"
    RERANKER_MODEL = "BAAI/bge-reranker-large"
    API_KEY = "c8bc2c37-95f2-4202-b12d-f9392185408f"
    BASE_URL = "https://ark.cn-beijing.volces.com/api/v3"
    MAX_SEQ_LENGTH = 512

    # æ•°æ®åº“é…ç½®
    COLLECTION_NAME = "DocumentDB"
    INSERT_BATCH = 500

    MODELS = {
      "DeepSeek_R1": "deepseek-r1-250120",
      "DeepSeek_V3": "deepseek-v3-250324"
    }
    MAX_TOKENS = 3000
    TEMPERATURE=0.7

"""# File Processor

## PDF Processor
"""

# %% ##########################
# PDFå¤„ç†æ¨¡å—
###############################
class PDFProcessor:
    @staticmethod
    def pdf_to_text(file_path: str) -> str:
        """å°†PDFæ–‡ä»¶è½¬æ¢ä¸ºçº¯æ–‡æœ¬"""
        try:
            text = []
            with fitz.open(file_path) as doc:
                for page in doc:
                    text.append(page.get_text())
            return "\n".join(text)
        except Exception as e:
            print(f"PDFè§£æå¤±è´¥ï¼š{file_path}\né”™è¯¯ï¼š{str(e)}")
            return ""

    @staticmethod
    def clean_text(text: str) -> str:
        """æ–‡æœ¬æ¸…æ´—å¤„ç†"""
        text = re.sub(r'\n+', ' ', text)  # åˆå¹¶æ¢è¡Œç¬¦
        text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä½™ç©ºæ ¼
        return text.strip()

    @staticmethod
    def batch_process_pdfs(folder_path: str) -> Dict[str, str]:
        """æ‰¹é‡å¤„ç†æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶"""
        pdf_texts = {}
        folder_path = Path(folder_path)

        if not folder_path.is_dir():
            raise ValueError(f"æ— æ•ˆçš„æ–‡ä»¶å¤¹è·¯å¾„: {folder_path}")

        # æ”¯æŒé€’å½’éå†ï¼ˆå¯é€‰ï¼‰
        for pdf_file in folder_path.glob("**/*.pdf"):
            try:
                text = PDFProcessor.pdf_to_text(str(pdf_file))
                cleaned = PDFProcessor.clean_text(text)
                pdf_texts[pdf_file.name] = cleaned
                print(f"æˆåŠŸå¤„ç†: {pdf_file.name}")
            except Exception as e:
                print(f"å¤„ç†å¤±è´¥ {pdf_file.name}: {str(e)}")
                continue

        return pdf_texts

"""### Test"""

text_from_pdf = PDFProcessor.pdf_to_text("/content/drive/MyDrive/513/Maotai_2024.pdf")

text_from_pdf_cleaned = PDFProcessor.clean_text(text_from_pdf)

"""## Text Chunker"""

# %% ##########################
# æ–‡æœ¬åˆ†å—æ¨¡å—
###############################
class TextChunker:

    @staticmethod
    def split_into_chunks(text: str, chunk_size: int) -> List[str]:
      """åŸºäºå¥å­çš„åˆ†å‰²"""
      sentences = sent_tokenize(text)
      chunks = []
      current_chunk = []
      current_length = 0

      for sent in sentences:
          sent_len = len(sent)

          # å¤„ç†è¶…é•¿å•ä¸ªå¥å­
          if sent_len > chunk_size:
              if current_chunk:  # å…ˆä¿å­˜å·²æœ‰å—
                  chunks.append(" ".join(current_chunk))
                  current_chunk = []
              # å¼ºåˆ¶åˆ†å‰²é•¿å¥
              chunks.extend([sent[i:i+chunk_size] for i in range(0, sent_len, chunk_size)])
              current_length = 0
              continue

          # æ­£å¸¸ç´¯åŠ é€»è¾‘
          if current_length + sent_len > chunk_size:
              chunks.append(" ".join(current_chunk))
              # æ™ºèƒ½é‡å ï¼ˆä¿ç•™å®Œæ•´å¥å­ï¼‰
              overlap = []
              overlap_len = 0
              while current_chunk and (overlap_len + len(current_chunk[-1])) <= chunk_size * 0.2:
                  overlap.insert(0, current_chunk.pop())
                  overlap_len += len(overlap[0])
              current_chunk = overlap
              current_length = overlap_len

          current_chunk.append(sent)
          current_length += sent_len

      if current_chunk:
          chunks.append(" ".join(current_chunk))

      return chunks


    # åœ¨TextChunkerç±»ä¸­å¯æ·»åŠ è‡ªé€‚åº”åˆ†å—ç®—æ³•

    @staticmethod
    def dynamic_chunking(text: str, min_size: int = 200, max_size: int = 500) -> List[str]:
        """
        ä¼˜åŒ–çš„åŠ¨æ€åˆ†å—ç®—æ³•(æ”¯æŒä¸­è‹±æ–‡é‡‘èæ–‡æ¡£)
        Args:
            text: è¾“å…¥æ–‡æœ¬(éœ€é¢„å…ˆæ¸…æ´—)
            min_size: æœ€å°åˆ†å—é•¿åº¦(å»ºè®®200-300)
            max_size: æœ€å¤§åˆ†å—é•¿åº¦(å»ºè®®500-800)
        Returns:
            List[str]: ç¬¦åˆé•¿åº¦é™åˆ¶çš„è¯­ä¹‰å—åˆ—è¡¨
        """
        if not text:
            return []

        # ä¼˜å…ˆçº§æ’åºçš„æ–­ç‚¹ç¬¦å·(åˆ†æ•°ï¼Œç¬¦å·é•¿åº¦)
        BREAKERS = {
            '\n\n': (0.95, 2),   # æ®µè½åˆ†éš”
            'ã€‚': (0.85, 1),     # å¥å­ç»“æŸ
            '!': (0.8, 1), '?': (0.8, 1),  # æ„Ÿå¹/ç–‘é—®
            'ï¼›': (0.7, 1), ';': (0.7, 1), # åˆ†å·
            'ï¼Œ': (0.5, 1), ',': (0.5, 1) # é€—å·
        }

        chunks = []
        start = 0
        text_len = len(text)

        while start < text_len:
            # è®¡ç®—å½“å‰çª—å£è¾¹ç•Œ
            end = min(start + max_size, text_len)
            window = text[start:end]

            best_split = -1
            best_score = 0

            # é€†å‘æ‰«æå¯»æ‰¾æœ€ä¼˜æ–­ç‚¹
            for pos in range(len(window)-1, min_size-1, -1):
                # è·å–å½“å‰ä½ç½®ä¸Šä¸‹æ–‡
                curr_char = window[pos]
                prev_char = window[pos-1] if pos > 0 else ''
                next_char = window[pos+1] if pos < len(window)-1 else ''

                # è¯„ä¼°æ‰€æœ‰å¯èƒ½çš„æ–­ç‚¹
                for br, (score, br_len) in BREAKERS.items():
                    # åŒ¹é…å¤šå­—ç¬¦æ–­ç‚¹
                    if br_len == 2 and pos >= 1:
                        if window[pos-1:pos+1] == br:
                            current_score = score
                            split_pos = pos + 1  # åŒ…å«æ–­ç‚¹ç¬¦å·
                            break
                    elif br_len == 1 and curr_char == br:
                        # å¤„ç†æ•°å­—ä¸­çš„é€—å·(å¦‚1,000)
                        if br in [',', 'ï¼Œ'] and next_char.isdigit():
                            continue
                        current_score = score
                        split_pos = pos + 1  # åŒ…å«æ–­ç‚¹ç¬¦å·
                        break
                else:
                    continue  # æœªæ‰¾åˆ°æ–­ç‚¹ç¬¦å·

                # æ›´æ–°æœ€ä½³åˆ†å‰²ç‚¹
                if current_score > best_score:
                    best_score = current_score
                    best_split = start + split_pos
                    if best_score >= 0.8:  # é«˜ä¼˜å…ˆçº§æ–­ç‚¹ç«‹å³ç¡®å®š
                        break

            # ç¡®å®šæœ€ç»ˆåˆ†å‰²ä½ç½®
            if best_split > start + min_size:
                chunks.append(text[start:best_split].strip())
                start = best_split
            else:
                # å¤„ç†å‰©ä½™æ–‡æœ¬ä¸è¶³min_sizeçš„æƒ…å†µ
                if chunks and (len(window) < min_size):
                    chunks[-1] += window  # åˆå¹¶åˆ°å‰ä¸€ä¸ªå—
                else:
                    chunks.append(window.strip())
                start = end

            # åŠ¨æ€é‡å å¤„ç†(ä¸è¶…è¿‡max_sizeçš„20%)
            overlap = min(int(max_size * 0.2), start - chunks[-1].rfind('\n\n') if '\n\n' in chunks[-1] else 50)
            start = max(start - overlap, 0)

        return [chunk for chunk in chunks if chunk]

"""### Test"""

text_chunks = TextChunker.split_into_chunks(text_from_pdf_cleaned, Config.CHUNK_SIZE)

print(len(text_chunks))

print(text_chunks[0])

dynamic_text_chunks = TextChunker.dynamic_chunking(text_from_pdf_cleaned)

print(len(dynamic_text_chunks))

print(dynamic_text_chunks[0])

"""# Vector DB

ä¿®æ”¹æƒé™ é˜²æ­¢æ— æ³•å¯¹åŸæ¥çš„æ•°æ®åº“è¿›è¡Œæ’å…¥
"""

!find /content/drive/MyDrive/513/chroma_db/ -name "*.lock" -delete

!chmod -R 775 /content/drive/MyDrive/513/chroma_db/

# %load_ext line_profiler

# %% ##########################
# å‘é‡æ•°æ®åº“æ¨¡å—
###############################
class VectorDBManager:
    #åˆå§‹åŒ–æ¨¡å—
    def __init__(self):
        self.pdf_texter = PDFProcessor()
        self.chunker = TextChunker()
        self.client = self._init_client()
        self.embedder = self._init_embedder()
        self.collection = self._init_collection()
        self.insert_executor = ThreadPoolExecutor(max_workers=Config.NUM_WORKERS)

    def _init_embedder(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆå¯å¹¶è¡Œä¼˜åŒ–ç‚¹ï¼‰"""
        model = SentenceTransformer(Config.EMBEDDING_MODEL)
        model.max_seq_length = Config.MAX_SEQ_LENGTH
        return model

    def _init_client(self):
        settings = chromadb.config.Settings(
          persist_directory=Config.DB_PATH,
          anonymized_telemetry=False,
          allow_reset=True,  # å…è®¸é‡ç½®æ•°æ®åº“
          # enable_lock=False   # ç¦ç”¨æ–‡ä»¶é”ï¼ˆé’ˆå¯¹Colabç¯å¢ƒï¼‰
          )

        return chromadb.PersistentClient(settings=settings)


    def _init_collection(self):
        """åˆå§‹åŒ–æ•°æ®åº“é›†åˆ"""


        embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=Config.EMBEDDING_MODEL,
            device="cuda",
            normalize_embeddings=True
        )
        return self.client.get_or_create_collection(
            name=Config.COLLECTION_NAME,
            embedding_function=embedding_func,
            metadata={"hnsw:space": "cosine"},
        )

    #ç”Ÿæˆå‘é‡
    def pdf_processor(self, pdf_path: str):
        # æ­¥éª¤1ï¼šPDFè½¬æ–‡æœ¬
        print("æ­£åœ¨è§£æPDF...")
        raw_text = self.pdf_texter.pdf_to_text(pdf_path)
        if not raw_text:
          print("PDFè§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
          return

        # æ­¥éª¤2ï¼šæ–‡æœ¬æ¸…æ´—å’Œåˆ†å—
        cleaned_text = self.pdf_texter.clean_text(raw_text)
        chunks = self.chunker.split_into_chunks(cleaned_text, Config.CHUNK_SIZE)
        chunks = [c for c in chunks if len(c.strip()) > 0]  # è¿‡æ»¤ç©ºå—
        print(f"ç”Ÿæˆæœ‰æ•ˆæ–‡æœ¬å—ï¼š{len(chunks)} ä¸ª")

        # æ­¥éª¤3ï¼šç”Ÿæˆå‘é‡ï¼ˆå¯å¹¶è¡Œä¼˜åŒ–ç‚¹ï¼‰
        print("ç”ŸæˆåµŒå…¥å‘é‡...")
        embeddings = []
        metadata = []
        doc_id = Path(pdf_path).stem  # è·å–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰

        if Config.USE_MULTITHREAD:
          #å¤šçº¿ç¨‹ç‰ˆæœ¬
          print("å¤šçº¿ç¨‹ç‰ˆæœ¬\n")
          # ä¿®æ”¹åçš„å‘é‡ç”Ÿæˆéƒ¨åˆ†
          def process_batch(batch, start_idx, doc_id):
              """å•æ‰¹æ¬¡å¤„ç†å‡½æ•°ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
              # ç”Ÿæˆå‘é‡
              vectors = self.embedder.encode(
                  batch,
                  convert_to_tensor=True,
                  normalize_embeddings=True,
                  show_progress_bar=False
              ).cpu().numpy()

              # æ„å»ºå…ƒæ•°æ®
              batch_meta = [{
                  "doc_id": doc_id,
                  "chunk_index": start_idx + idx,
                  "text_snippet": chunk[:100] + "...",
              } for idx, chunk in enumerate(batch)]

              return vectors, batch_meta

          with ThreadPoolExecutor(max_workers=Config.NUM_WORKERS) as executor:
            # 1. å‡†å¤‡ä»»åŠ¡é˜Ÿåˆ—
            futures = []
            for i in range(0, len(chunks), Config.BATCH_SIZE):
                batch = chunks[i:i+Config.BATCH_SIZE]
                # æäº¤ä»»åŠ¡ï¼ˆç«‹å³éé˜»å¡ï¼‰
                future = executor.submit(
                    process_batch,
                    batch=batch,
                    start_idx=i,
                    doc_id=doc_id
                )
                futures.append(future)

            # 2. å¹¶è¡Œå¤„ç† + è¿›åº¦è·Ÿè¸ª
            for future in tqdm(as_completed(futures),
                              total=len(futures),
                              desc="å¤šçº¿ç¨‹ç”Ÿæˆå‘é‡"):
                batch_vectors, batch_meta = future.result()
                embeddings.extend(batch_vectors)
                metadata.extend(batch_meta)
        else:
          #å•çº¿ç¨‹ç‰ˆæœ¬
          print("å•çº¿ç¨‹ç‰ˆæœ¬\n")
          for i in tqdm(range(0, len(chunks), Config.BATCH_SIZE), desc="å‘é‡ç”Ÿæˆ"):
              batch = chunks[i:i+Config.BATCH_SIZE]

              # ç”Ÿæˆå‘é‡
              vectors = self.embedder.encode(
                  batch,
                  convert_to_tensor=True,
                  normalize_embeddings=True,
                  show_progress_bar=False
              ).cpu().numpy()

              # æ”¶é›†æ•°æ®
              embeddings.extend(vectors)
              metadata.extend([{
                  "doc_id": doc_id,
                  "chunk_index": i + idx,
                  "text_snippet": chunk[:100] + "...",  # æˆªå–å‰100å­—ç¬¦
              } for idx, chunk in enumerate(batch)])

        # æ­¥éª¤4ï¼šå­˜å‚¨åˆ°æ•°æ®åº“
        print("\næ­£åœ¨å†™å…¥æ•°æ®åº“...")

        if Config.USE_MULTITHREAD:
          self.insert_data_parallel(chunks, embeddings, metadata)
        else:
          self.insert_data(chunks, embeddings, metadata)

    #å‘é‡æ’å…¥
    @retry(stop=stop_after_attempt(3),
           wait=wait_exponential(multiplier=1, min=2, max=10))
    def _insert_batch(self, batch: dict):
        try:
          self.collection.add(**batch)
          return True
        except Exception as e:
          print(f"æ’å…¥é‡è¯•ä¸­... é”™è¯¯ï¼š{str(e)}")
          raise

    def insert_data(self, chunks: List[str], embeddings: List[np.ndarray], metadata: List[Dict]):
        """æ‰¹é‡æ’å…¥æ•°æ®ï¼ˆåŒ…å«å®Œæ•´æ–‡æœ¬å†…å®¹ï¼‰

        å‚æ•°è¯´æ˜ï¼š
        - chunks: æ–‡æœ¬å—åˆ—è¡¨ï¼ˆåŸå§‹æ–‡æœ¬å†…å®¹ï¼‰
        - embeddings: å¯¹åº”çš„å‘é‡åˆ—è¡¨
        - metadata: å…ƒæ•°æ®åˆ—è¡¨ï¼ˆéœ€åŒ…å«doc_idç­‰ä¿¡æ¯ï¼‰
        """
        # æ•°æ®æ ¡éªŒï¼ˆç¡®ä¿ä¸‰ä¸ªåˆ—è¡¨é•¿åº¦ä¸€è‡´ï¼‰
        assert len(chunks) == len(embeddings) == len(metadata), "æ•°æ®é•¿åº¦ä¸ä¸€è‡´"

        # å‡†å¤‡æ•°æ®æ ¼å¼ï¼ˆå…³é”®ä¿®æ”¹ç‚¹ï¼‰
        ids = [f"{m['doc_id']}_{m['chunk_index']}" for m in metadata]  # å”¯ä¸€ID
        documents = chunks  # ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬å—ä½œä¸ºæ–‡æ¡£å†…å®¹

        # åˆ†æ‰¹æ¬¡æ’å…¥ï¼ˆæ˜¾ç¤ºè¿›åº¦æ¡ï¼‰
        for i in tqdm(range(0, len(ids), Config.INSERT_BATCH),
                    desc="æ•°æ®åº“æ’å…¥",
                    unit="batch"):
            batch = {
                "ids": ids[i:i+Config.INSERT_BATCH],
                "embeddings": embeddings[i:i+Config.INSERT_BATCH],
                "documents": documents[i:i+Config.INSERT_BATCH],  # å…³é”®ä¿®æ”¹
                "metadatas": metadata[i:i+Config.INSERT_BATCH]
            }
            try:
                self.collection.add(**batch)
            except Exception as e:
                print(f"æ’å…¥å¤±è´¥æ‰¹æ¬¡ {i}-{i+Config.INSERT_BATCH}: {str(e)}")

        print(f"æ’å…¥å®Œæˆï¼Œæ€»æ•°æ®é‡ï¼š{self.collection.count()}")

    def insert_data_parallel(self, chunks: List[str], embeddings: List[np.ndarray], metadata: List[Dict]):
        """å¹¶è¡ŒåŒ–æ‰¹é‡æ’å…¥"""
        assert len(chunks) == len(embeddings) == len(metadata)

        # å‡†å¤‡æ‰€æœ‰æ‰¹æ¬¡æ•°æ®ï¼ˆé¿å…åœ¨å¹¶è¡Œä¸­ä¿®æ”¹ï¼‰
        batch_list = []
        ids = [f"{m['doc_id']}_{m['chunk_index']}" for m in metadata]
        for i in range(0, len(ids), Config.INSERT_BATCH):
            batch_list.append({
                "ids": ids[i:i+Config.INSERT_BATCH],
                "embeddings": embeddings[i:i+Config.INSERT_BATCH],
                "documents": chunks[i:i+Config.INSERT_BATCH],
                "metadatas": metadata[i:i+Config.INSERT_BATCH]
            })

        # å¹¶è¡Œæäº¤ä»»åŠ¡
        futures = []
        for batch in batch_list:
            futures.append(self.insert_executor.submit(self._insert_batch, batch))

        # ç›‘æ§è¿›åº¦ä¸é”™è¯¯
        success_count = 0
        with tqdm(as_completed(futures), total=len(batch_list), desc="å¹¶è¡Œæ’å…¥") as pbar:
            for future in pbar:
                try:
                    if future.result():
                        success_count += 1
                    pbar.set_postfix_str(f"æˆåŠŸ: {success_count}/{len(batch_list)}")
                except Exception as e:
                    print(f"\næ°¸ä¹…æ’å…¥å¤±è´¥: {str(e)}")

        print(f"æ’å…¥å®Œæˆ - æˆåŠŸæ‰¹æ¬¡: {success_count}/{len(batch_list)}")

    def search(self, query: str, top_k: int = Config.TOP_K) -> List[Tuple[str, float, dict]]:
        """å¢å¼ºå‹å‘é‡æœç´¢ï¼ˆè¿”å›å®Œæ•´å…ƒæ•°æ®ï¼‰"""
        results = self.collection.query(
            query_texts=[query],
            n_results=top_k,
            include=["documents", "distances", "metadatas"]
        )
        return list(zip(
            results["documents"][0],
            results["distances"][0],
            results["metadatas"][0]
        ))

    # èµ„æºæ£€æŸ¥
    # def _check_resources(self):
    #     """ç³»ç»Ÿèµ„æºæ£€æŸ¥"""
    #     # å­˜å‚¨ç©ºé—´æ£€æŸ¥
    #     free_space = psutil.disk_usage(Config.DB_PATH).free
    #     if free_space < 1 * 1024**3:  # 1GB
    #         raise RuntimeError("å­˜å‚¨ç©ºé—´ä¸è¶³ï¼Œéœ€è¦è‡³å°‘1GBå¯ç”¨ç©ºé—´")

    #     # å†…å­˜æ£€æŸ¥
    #     mem = psutil.virtual_memory()
    #     if mem.available < 2 * 1024**3:  # 2GB
    #         raise RuntimeError("å†…å­˜ä¸è¶³ï¼Œéœ€è¦è‡³å°‘2GBå¯ç”¨å†…å­˜")

    def process_folder(self, folder_path: str):
        """æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„PDFæ–‡ä»¶"""
        print(f"å¼€å§‹æ‰¹é‡å¤„ç†æ–‡ä»¶å¤¹: {folder_path}")
        start_time = time.time()

        # è·å–æ‰€æœ‰PDFæ–‡ä»¶
        pdf_files = [f for f in Path(folder_path).glob("*.pdf") if f.is_file()]
        if not pdf_files:
            raise FileNotFoundError("æœªæ‰¾åˆ°PDFæ–‡ä»¶")

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=Config.NUM_WORKERS) as executor:
            futures = []
            for pdf_file in pdf_files:
                future = executor.submit(self.pdf_processor, str(pdf_file))
                futures.append(future)

            # å¤„ç†å®Œæˆç»Ÿè®¡
            success = 0
            for future in tqdm(as_completed(futures), total=len(futures), desc="æ‰¹é‡å¤„ç†"):
                try:
                    future.result()
                    success += 1
                except Exception as e:
                    print(f"å¤„ç†å¤±è´¥: {str(e)}")

        print(f"å¤„ç†å®Œæˆ æˆåŠŸ: {success}/{len(pdf_files)}")
        print(f"æ€»è€—æ—¶: {time.time()-start_time:.2f}ç§’")

"""## Test"""

text_db = VectorDBManager()

text_db.pdf_processor("/content/drive/MyDrive/513/Maotai_2024.pdf")

def process_pdfs(folder_path: str):

    files = [f for f in os.listdir(folder_path)]

    for file in files:
            try:
                # æ„å»ºå®Œæ•´è·¯å¾„
                file_path = os.path.join(folder_path, file)
                text_db.pdf_processor(file_path)
            except Exception as e:
                print(f"å¤„ç†å¤±è´¥ï¼š{file} | é”™è¯¯ï¼š{str(e)}")

process_pdfs("/content/drive/MyDrive/513/A50_pdfs_named/")

from google.colab import runtime
runtime.unassign()

answers = text_db.search("è´µå·èŒ…å°2024å¹´å‡€åˆ©æ¶¦")

print(len(answers))

print(answers[0])

"""# Retriever"""

# =============================
# æ£€ç´¢ç»“æœå¤„ç†æ¨¡å—
# =============================
class RetrievalProcessor:
    """å¤„ç†æ£€ç´¢ç»“æœçš„é‡æ’åºå’Œè¿‡æ»¤"""
    def __init__(self):
        try:
            self.reranker = CrossEncoder(Config.RERANKER_MODEL)
            self.bm25 = None  # å»¶è¿Ÿåˆå§‹åŒ–
        except Exception as e:
            print(f"RetrievalProcessoråˆå§‹åŒ–é”™è¯¯:{str(e)}")
            raise

    def process_results(self, query: str, results: List[Tuple[str, float, dict]]):
        """
        å¤„ç†æ£€ç´¢ç»“æœçš„ä¸‰é˜¶æ®µæµç¨‹ï¼š
        1. BM25è¿‡æ»¤ -> 2.é‡æ’åº -> 3.å…ƒæ•°æ®è¿‡æ»¤
        """
        # é˜¶æ®µ1ï¼šBM25è¿‡æ»¤
        try:
            filtered_indices = self._bm25_filter(query, [r[0] for r in results])
            filtered_results = [results[i] for i in filtered_indices]  # æ–°å¢ç»“æœè¿‡æ»¤
        except Exception as e:
            return f"å¤„ç†æ£€ç´¢ç»“æœçš„é˜¶æ®µ1ï¼šBM25è¿‡æ»¤é”™è¯¯:{str(e)}"

        # é˜¶æ®µ2ï¼šé‡æ’åº
        try:
            # ä¿®æ­£ç‚¹ï¼šä¼ å…¥åŸå§‹ç»“æœå’Œè¿‡æ»¤åçš„ç´¢å¼•
            reranked = self._rerank(query, filtered_results, filtered_indices)
        except Exception as e:
            return f"é˜¶æ®µ2ï¼šé‡æ’åºé”™è¯¯:{str(e)}"

        # é˜¶æ®µ3ï¼šå…ƒæ•°æ®è¿‡æ»¤
        try:
            final_results = self._metadata_filter(reranked)
        except Exception as e:
            return f"é˜¶æ®µ3ï¼šå…ƒæ•°æ®è¿‡æ»¤é”™è¯¯:{str(e)}"

        return final_results[:Config.TOP_K_FINAL]

    def _bm25_filter(self, query: str, documents: List[str]) -> List[int]:
        """ä½¿ç”¨BM25è¿›è¡Œå…³é”®è¯è¿‡æ»¤"""
        # åˆå§‹åŒ–BM25
        try:
            # æ·»åŠ ç©ºæ–‡æ¡£æ£€æŸ¥
            valid_docs = [doc for doc in documents if len(doc.strip()) > 0]
            if not valid_docs:
                return []

            tokenized_docs = [jieba.lcut_for_search(doc) for doc in valid_docs]
            self.bm25 = BM25Okapi(tokenized_docs)
        except Exception as e:
            raise Exception(f"åˆå§‹åŒ–BM25é”™è¯¯:{str(e)}")

        # æå–æŸ¥è¯¢å…³é”®è¯
        try:
            query_terms = jieba.lcut_for_search(query.strip())
            if not query_terms:
                return []
        except Exception as e:
            raise Exception(f"æå–æŸ¥è¯¢å…³é”®è¯é”™è¯¯:{str(e)}")

        # è·å–BM25åˆ†æ•°
        try:
            scores = self.bm25.get_scores(query_terms)
        except Exception as e:
            raise Exception(f"è·å–BM25åˆ†æ•°é”™è¯¯:{str(e)}")

        # é€‰å–å‰50%çš„æ–‡æ¡£ï¼ˆä¿®æ­£ç´¢å¼•æ˜ å°„ï¼‰
        threshold = np.percentile(scores, 50)
        return [i for i, s in enumerate(scores) if s >= threshold]

    def _rerank(self, query: str, filtered_results: List[Tuple], indices: List[int]) -> List[Tuple]:
        """ä½¿ç”¨é‡æ’åºæ¨¡å‹ä¼˜åŒ–ç»“æœ"""
        # ä¿®æ­£ç‚¹ï¼šä½¿ç”¨è¿‡æ»¤åçš„ç»“æœæ•°æ®
        pairs = [(query, res[0]) for res in filtered_results]  # âœ… ä½¿ç”¨ä¼ å…¥çš„filtered_results
        try:
            scores = self.reranker.predict(pairs, batch_size=32)
        except Exception as e:
            raise Exception(f"é‡æ’åºæ¨¡å‹é¢„æµ‹é”™è¯¯:{str(e)}")

        # ç»„åˆç»“æœæ—¶ä¿ç•™åŸå§‹å…ƒæ•°æ®
        reranked = sorted(
            zip(filtered_results, scores),
            key=lambda x: -x[1]
        )
        return reranked

    def _metadata_filter(self, results: List[Tuple]) -> List[Tuple]:
        """åŸºäºå…ƒæ•°æ®çš„è¿‡æ»¤ï¼ˆä¿®æ­£æ–‡æœ¬é•¿åº¦æ£€æŸ¥ï¼‰"""
        # ä¿®æ­£ç‚¹ï¼šæ­£ç¡®è®¿é—®æ–‡æœ¬å†…å®¹
        return [r for r in results if len(r[0][0].strip()) >= 50]  # âœ… r[0][0]å¯¹åº”åŸå§‹æ–‡æœ¬å†…å®¹

"""## Test"""

test_retriever = RetrievalProcessor()

final_answers = test_retriever.process_results("è´µå·èŒ…å°2024å¹´å‡€åˆ©æ¶¦", answers)

print(len(final_answers))

print(final_answers[:])

print(final_answers[0][0])

"""# Chat System"""

class ChatSystem:
    def __init__(self):
        self.client = OpenAI(
            base_url=Config.BASE_URL,
            api_key=Config.API_KEY
        )

    def generate_response(self, context: str, query: str, model_name: str) -> str:
        """ç”Ÿæˆæ™ºèƒ½å›å¤"""
        prompt = f"""
        åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­å¾—åˆ°ç­”æ¡ˆï¼Œè¯·è¯´æ˜åŸå› ã€‚

        ä¸Šä¸‹æ–‡ï¼š
        {context}

        é—®é¢˜ï¼š{query}
        """
        try:
          response = self.client.chat.completions.create(
              model=model_name,
              messages=[
                  {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¢åŠ¡æŠ¥è¡¨åˆ†æäººå·¥æ™ºèƒ½åŠ©æ‰‹,è¯·é€‚å½“é‡‡ç”¨è¯­æ³•é«˜äº®å¯¹è¾“å‡ºè¿›è¡Œç¾åŒ–"},
                  {"role": "user", "content": prompt}
              ],
              temperature=Config.TEMPERATURE,
              max_tokens=Config.MAX_TOKENS
          )
        except Exception as e:
          return f"{str(e)}"

        return response.choices[0].message.content

"""## Test"""

chat = ChatSystem()

chat.generate_response(final_answers[0][0], "è´µå·èŒ…å°2024å¹´å‡€åˆ©æ¶¦", "deepseek-r1-250120")

"""# User Interface"""

# =============================
# äº¤äº’ç•Œé¢æ¨¡å—
# =============================
class ChatInterface:
    """Gradioäº¤äº’ç•Œé¢ç®¡ç†"""
    def __init__(self):
      try:
         self.db = VectorDBManager()
         self.retriever = RetrievalProcessor()
         self.chat_system = ChatSystem()
         self._setup_ui()
      except Exception as e:
        print(f"ç•Œé¢åˆå§‹åŒ–å¤±è´¥:{str(e)}")
        raise

    # åœ¨ChatInterfaceç±»ä¸­ä¿®æ”¹handle_uploadæ–¹æ³•
    def handle_upload(self, file_obj) -> str:
        """æ”¯æŒå¤„ç†æ–‡ä»¶å¤¹ä¸Šä¼ """
        try:
            if isinstance(file_obj, list):  # Gradioæ–‡ä»¶å¤¹ä¸Šä¼ ä¼šè¿”å›åˆ—è¡¨
                for f in file_obj:
                    self.db.pdf_processor(f.name)
                return f"æˆåŠŸå¤„ç† {len(file_obj)} ä¸ªæ–‡ä»¶"
            else:
                self.db.pdf_processor(file_obj.name)
                return f"æˆåŠŸå¤„ç†: {Path(file_obj.name).name}"
        except Exception as e:
            return f"å¤„ç†å¤±è´¥: {str(e)}"

    def _setup_ui(self):
        """æ„å»ºäº¤äº’ç•Œé¢"""
        with gr.Blocks(title="æ™ºèƒ½æ–‡æ¡£åŠ©æ‰‹ğŸ¤–") as self.interface:
          with gr.Row():
            with gr.Column(scale=1):
              # # å¤šæ–‡ä»¶/æ–‡ä»¶å¤¹ä¸Šä¼ 
              # self.file_upload = gr.File(
              #     label="ä¸Šä¼ PDFæ–‡æ¡£/æ–‡ä»¶å¤¹",
              #     file_count="directory"  # æ”¯æŒæ–‡ä»¶å¤¹ä¸Šä¼ 
              # )
              self.file_upload = gr.File(label="ä¸Šä¼ PDFæ–‡æ¡£")
              self.upload_btn = gr.Button("å¤„ç†æ–‡æ¡£")
              self.status = gr.Textbox(label="å¤„ç†çŠ¶æ€")

            with gr.Column(scale=2):
              # æ¨¡å‹é€‰æ‹©ç»„ä»¶
              self.model_selector = gr.Dropdown(
                  label="é€‰æ‹©æ¨¡å‹",
                  info="Deepseek-R1:æ·±åº¦æ¨ç† | Deepseek-V3:å¿«é€Ÿå“åº”",
                  choices=list(Config.MODELS.keys()),
                  value="Deepseek-V3"
              )


              # é—®ç­”ç»„ä»¶
              self.query_input = gr.Textbox(label="è¾“å…¥é—®é¢˜")
              self.search_btn = gr.Button("å¼€å§‹æŸ¥è¯¢")
              self.context_display = gr.Textbox(label="æ£€ç´¢ä¸Šä¸‹æ–‡")
              self.answer_output = gr.Textbox(label="æ™ºèƒ½å›å¤")

            # äº‹ä»¶ç»‘å®š
            self.upload_btn.click(
                self.handle_upload,
                inputs=self.file_upload,
                outputs=self.status
            )
            self.search_btn.click(
                self.handle_query,
                inputs=[self.query_input, self.model_selector],
                outputs=[self.context_display, self.answer_output]
            )

    def handle_upload(self, file_path: str) -> str:
        """å¤„ç†æ–‡ä»¶ä¸Šä¼ """
        if not file_path:
            return "è¯·é€‰æ‹©PDFæ–‡ä»¶"

        try:
          self.db.pdf_processor(file_path)
          return f"âœ… æˆåŠŸå¤„ç†: {Path(file_path).name}"
        except Exception as e:
          return f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼. {str(e)}"

    def handle_query(self, query: str, model: str) -> Tuple[str, str]:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        model_id = Config.MODELS.get(model)

        # å‘é‡æœç´¢
        raw_results = self.db.search(query)

        # ç»“æœå¤„ç†
        processed = self.retriever.process_results(query, raw_results)

        context = "\n".join([r[0][0] for r in processed])

        # ç”Ÿæˆæ™ºèƒ½å›å¤
        response = self.chat_system.generate_response(context, query, model_id)

        return context, response

"""## Test"""

UI = ChatInterface()

UI.interface.launch()