# -*- coding: utf-8 -*-
"""pdf_downloader.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Th48U9GTckIxX9x6Gyu_8vq4JNTGFocf
"""

!pip install pandas openpyxl akshare

import akshare as ak
import pandas as pd
import os
from tqdm import tqdm  # 进度条支持

from google.colab import drive
drive.mount('/content/drive')

"""# 找出每个公司过去的年报pdf"""

pre_dir="./"
symbol_str = '''688981,603993,603259,601899,601888,601816,601766,601668,601600,601318,601088,601012,600941,600900,600893,600887,600660,600585,600519,600436,600426,600415,600406,600309,600276,600176,600036,600031,600030,600028,600019,600009,300760,300750,300408,300124,300122,300015,002714,002594,002475,002371,002230,002027,000938,000792,000725,000333,000063,000002'''

def download_annual_reports(symbol_str: str, pre_dir: str):
    """
    批量下载股票年报数据

    参数：
    symbol_str : str - 逗号分隔的股票代码字符串
    pre_dir    : str - 文件保存目录，默认当前目录

    返回：
    error_list : list - 下载失败的股票代码列表
    """
    # 创建存储目录
    os.makedirs(pre_dir, exist_ok=True)

    # 拆分股票代码并去重
    symbols = list(set(symbol_str.split(',')))
    # print(symbols)
    error_list = []

    # 带进度条的遍历
    for symbol in tqdm(symbols, desc="下载年报数据"):
        try:
            # print(symbol)
            df = ak.stock_zh_a_disclosure_report_cninfo(symbol=symbol, market="沪深京",
                                                        category="年报",
                                                        start_date="20000101",
                                                        end_date="20250315")

            df.to_excel(pre_dir+symbol+'.xlsx',engine='openpyxl')
        except:
            print(symbol)
    pass

download_annual_reports(symbol_str, '/content/drive/MyDrive/513/A50/')

"""## 数据清洗
### 需要把年报公告中有‘摘要’，‘英文’的公告都去除掉
"""

def process_reports(folder_path: str, output_path: str):
    """
    批量处理文件夹中所有xlsx文件，保留纯年度报告数据
    :param folder_path: 包含xlsx文件的文件夹路径
    """
    # 获取文件列表（仅当前目录）
    files = [f for f in os.listdir(folder_path)
            if f.endswith('.xlsx') and not f.startswith('~$')]

    for file in files:
        try:
            # 构建完整路径
            file_path = os.path.join(folder_path, file)

            # 读取数据（网页6推荐的pandas方法）
            df = pd.read_excel(file_path, engine='openpyxl')

            # 核心筛选逻辑（网页7的布尔索引技巧）
            mask = (
                df['公告标题'].str.contains('年度报告', na=False) &
                ~df['公告标题'].str.contains('摘要|英文', regex=True, na=False)
            )
            filtered_df = df[mask]

            # 覆盖原文件保存（网页5的保存方式）
            filtered_df.to_excel(os.path.join(output_path, file), index=False, engine='openpyxl')

            print(f"成功处理：{file}")

        except Exception as e:
            print(f"处理失败：{file} | 错误：{str(e)}")

input_folder = "/content/drive/MyDrive/513/A50/"  # 原始文件目录
output_folder = "/content/drive/MyDrive/513/A50_filtered/"

process_reports(input_folder, output_folder)

"""# 把查看pdf的url转换成下载的url"""

def process_announcement_urls(source_dir: str, target_dir: str):
    """
    处理公告链接并生成PDF链接列表
    步骤：
    1. 遍历源目录中的Excel文件
    2. 从每个文件中提取公告链接
    3. 转换为PDF格式的URL
    4. 将结果保存到目标目录的文本文件中
    """
    # 配置路径（使用原始字符串防止转义）
    source_dir = source_dir
    target_dir = target_dir

    # 确保目标目录存在
    os.makedirs(target_dir, exist_ok=True)

    # 遍历源目录下所有文件
    for filename in os.listdir(source_dir):
        # 从文件名提取股票代码（前6位字符）
        ticker = filename[:6]

        try:
            # 构建完整文件路径并读取Excel
            file_path = os.path.join(source_dir, filename)
            df = pd.read_excel(file_path, engine='openpyxl')

            # 验证包含必要列
            if '公告链接' not in df.columns:
                print(f"文件 {filename} 缺少'公告链接'列，跳过处理")
                continue

            # 转换URL处理逻辑
            pdf_urls = []
            for url in df['公告链接'].tolist():
                # 参数安全解析
                params = url.split('&')
                if len(params) < 2:
                    print(f"非常规URL格式: {url}")
                    continue

                # 提取公告ID和时间参数
                try:
                    announcement_id = params[1].split('=')[1]  # 格式：announcementId=xxxxxx
                    raw_time = params[-1].split('=')[1]        # 格式：announcementTime=YYYY-MM-DD...
                    formatted_time = raw_time[:10]            # 取日期部分
                except IndexError as e:
                    print(f"参数解析失败: {url} ({e})")
                    continue

                # 生成标准PDF链接
                pdf_url = f'http://static.cninfo.com.cn/finalpage/{formatted_time}/{announcement_id}.PDF'
                pdf_urls.append(pdf_url)

            # 写入结果文件
            output_path = os.path.join(target_dir, f"{ticker}.txt")
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(pdf_urls))

            print(f"成功处理: {filename} => {ticker}.txt")

        except Exception as e:
            print(f"处理文件 {filename} 时发生错误: {str(e)}")
            continue

source_dir="/content/drive/MyDrive/513/A50_filtered/"
target_dir="/content/drive/MyDrive/513/A50_pdf_download_links/"

process_announcement_urls(source_dir, target_dir)

"""# 批量下载pdf"""

import os
from urllib.parse import urlparse

def generate_pdf_filename(url):
    """从指定格式的URL生成标准化文件名"""
    filename = os.path.basename(url).split('.')[0]
    print(filename)

    # 解析URL路径
    path = urlparse(url).path  # 示例：'/finalpage/2024-03-29/1219447098.PDF'
    print(path)

    # 分割路径获取日期和文件名
    path_parts = path.split('/')  # 示例：['', 'finalpage', '2024-03-29', '1219447098.PDF']
    print(path_parts)

    # 提取日期部分（路径第3段）
    date_str = path_parts[-2]  # 示例：'2024-03-29'
    print(date_str)

    # 提取公告ID（路径第4段，去除扩展名）
    filename_part = path_parts[-1]
    print(filename_part)
    announcement_id = os.path.splitext(filename_part)[0]  # 示例：'1219447098'

    # 生成标准化文件名
    return f"{date_str}_{announcement_id}"

# 使用示例
url = "http://static.cninfo.com.cn/finalpage/2024-03-29/1219447098.PDF"
filename = generate_pdf_filename(url)
print(filename)  # 输出：2024-03-29-1219447098.pdf

import requests
from urllib.parse import urlparse

def download_pdfs_from_txts(source_folder: str, target_folder: str):
    """
    批量下载TXT文件中的PDF链接
    :param source_folder: 存放TXT文件的源文件夹路径
    :param target_folder: PDF文件保存目录
    """
    # 创建目标文件夹[4,5](@ref)
    os.makedirs(target_folder, exist_ok=True)

    # 获取所有TXT文件
    txt_files = [f for f in os.listdir(source_folder)
                if f.endswith('.txt') and not f.startswith('~$')]

    total_downloaded = 0
    error_log = []

    for txt_file in tqdm(txt_files, desc="处理进度"):
        file_path = os.path.join(source_folder, txt_file)

        try:
            # 读取TXT文件并去重链接[5](@ref)
            with open(file_path, 'r', encoding='utf-8') as f:
                urls = list(set(line.strip() for line in f if line.strip()))

            for url in urls:
                try:
                    # 获取文件名[4](@ref)
                    path = urlparse(url).path  # 去除URL参数
                    path_parts = path.split('/')  # 去除URL参数odf
                    date_str = path_parts[-2]
                    filename_part = path_parts[-1]
                    filename = f"{date_str}_{filename_part}"

                    save_path = os.path.join(target_folder, filename)


                    # 流式下载大文件[6](@ref)
                    with requests.get(url, stream=True, timeout=10) as r:
                        r.raise_for_status()
                        with open(save_path, 'wb') as f:
                            for chunk in r.iter_content(chunk_size=8192):
                                f.write(chunk)
                    total_downloaded += 1
                except Exception as e:
                    error_log.append(f"{url} | 错误：{type(e).__name__}-{str(e)}")

        except Exception as e:
            error_log.append(f"文件 {txt_file} | 错误：{type(e).__name__}-{str(e)}")
            print(f"文件 {txt_file} | 错误：{type(e).__name__}-{str(e)}")

    # 输出统计结果
    print(f"\n下载完成！成功下载 {total_downloaded} 个PDF文件")
    print(f"保存目录：{os.path.abspath(target_folder)}")

    if error_log:
        print("\n错误日志：")
        for log in error_log[-10:]:  # 显示最后10条错误
            print(f"• {log}")

import os
import requests
import threading
from urllib.parse import urlparse
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

class ThreadSafeCounter:
    """线程安全计数器"""
    def __init__(self):
        self._value = 0
        self._lock = threading.Lock()

    def increment(self):
        with self._lock:
            self._value += 1

    @property
    def value(self):
        with self._lock:
            return self._value

def download_single_pdf(url, target_folder, error_queue, counter):
    """单个PDF下载线程"""
    try:
        # 生成唯一文件名
        path = urlparse(url).path
        path_parts = path.split('/')
        if len(path_parts) < 4:
            raise ValueError("URL路径格式错误")

        date_str = path_parts[-2]
        filename = f"{date_str}-{os.path.splitext(path_parts[-1])[0]}.pdf"
        save_path = os.path.join(target_folder, filename)

        # 检查文件是否已存在
        if os.path.exists(save_path):
            return  # 跳过已存在文件

        # 流式下载
        with requests.get(url, stream=True, timeout=(6.1, 30)) as r:
            r.raise_for_status()
            with open(save_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)

        # 线程安全计数器递增
        counter.increment()

    except Exception as e:
        error_queue.put(f"{url} | 错误：{type(e).__name__}-{str(e)}")

def download_pdfs_from_txts(source_folder: str, target_folder: str, max_workers=20):
    """
    多线程下载PDF主函数
    :param source_folder: TXT文件目录
    :param target_folder: PDF保存目录
    :param max_workers: 并发线程数（建议10-30）
    """
    # 初始化线程安全组件
    error_queue = Queue()
    counter = ThreadSafeCounter()

    # 创建目标目录
    os.makedirs(target_folder, exist_ok=True)

    # 收集所有下载链接
    all_urls = []
    for txt_file in os.listdir(source_folder):
        if not txt_file.endswith('.txt') or txt_file.startswith('~$'):
            continue

        with open(os.path.join(source_folder, txt_file), 'r', encoding='utf-8') as f:
            urls = list({line.strip() for line in f if line.strip()})
            all_urls.extend(urls)

    # 进度条配置
    with tqdm(total=len(all_urls), desc="总进度") as pbar:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # 提交下载任务
            futures = {
                executor.submit(
                    download_single_pdf,
                    url,
                    target_folder,
                    error_queue,
                    counter
                ): url for url in all_urls
            }

            # 更新进度条
            for _ in as_completed(futures):
                pbar.update(1)

    # 输出统计信息
    print(f"\n下载完成！成功下载 {counter.value}/{len(all_urls)} 个文件")
    print(f"保存目录：{os.path.abspath(target_folder)}")

    # 输出错误日志
    if not error_queue.empty():
        print("\n错误日志（最多显示20条）：")
        for _ in range(min(20, error_queue.qsize())):
            print(f"• {error_queue.get()}")

# 使用示例
download_pdfs_from_txts(
    source_folder="/content/drive/MyDrive/513/A50_pdf_download_links/",
    target_folder="/content/drive/MyDrive/513/A50_pdfs/",
    max_workers=4  # 根据网络带宽调整（建议值：10-30）
)

input_folder = "/content/drive/MyDrive/513/A50_pdf_download_links/"
output_folder = "/content/drive/MyDrive/513/A50_pdfs/"

download_pdfs_from_txts(input_folder, output_folder)

def download_pdfs_from_txts(source_folder: str, target_folder: str, max_workers=20):
    # [其他部分保持不变...]

import os
import requests
import threading
from urllib.parse import urlparse
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

class ThreadSafeCounter:
    """线程安全计数器"""
    def __init__(self):
        self._value = 0
        self._lock = threading.Lock()

    def increment(self):
        with self._lock:
            self._value += 1

    @property
    def value(self):
        with self._lock:
            return self._value

def download_single_pdf(url, target_folder, error_queue, counter, txt_name):
    """单个PDF下载线程（新增txt_name参数）"""
    try:
        path = urlparse(url).path
        path_parts = path.split('/')
        if len(path_parts) < 4:
            raise ValueError("URL路径格式错误")

        date_str = path_parts[-2]
        base_name = os.path.splitext(path_parts[-1])[0]

        # 添加TXT文件名前缀
        filename = f"{txt_name}_{date_str}-{base_name}.pdf"  # 新增文件名部分
        save_path = os.path.join(target_folder, filename)

        if os.path.exists(save_path):
            return

        with requests.get(url, stream=True, timeout=(6.1, 30)) as r:
            r.raise_for_status()
            with open(save_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)

        counter.increment()

    except Exception as e:
        error_queue.put(f"{url} | 错误：{type(e).__name__}-{str(e)}")

def download_pdfs_from_txts(source_folder: str, target_folder: str, max_workers=20):
    """
    多线程下载PDF主函数
    :param source_folder: TXT文件目录
    :param target_folder: PDF保存目录
    :param max_workers: 并发线程数（建议10-30）
    """
    # 初始化线程安全组件
    error_queue = Queue()
    counter = ThreadSafeCounter()

    # 创建目标目录
    os.makedirs(target_folder, exist_ok=True)

    # 收集所有下载链接
    # 修改URL收集逻辑
    all_urls = []
    for txt_file in os.listdir(source_folder):
        if not txt_file.endswith('.txt') or txt_file.startswith('~$'):
            continue

        # 获取TXT文件名（不含扩展名）
        txt_name = os.path.splitext(txt_file)[0]

        with open(os.path.join(source_folder, txt_file), 'r', encoding='utf-8') as f:
            urls = list({line.strip() for line in f if line.strip()})
            # 存储元组（URL，TXT文件名）
            all_urls.extend( [(url, txt_name) for url in urls] )  # 修改这里

    # 修改任务提交部分
    with tqdm(total=len(all_urls), desc="总进度") as pbar:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(
                    download_single_pdf,
                    url,           # URL参数
                    target_folder,
                    error_queue,
                    counter,
                    txt_name       # 新增TXT文件名参数
                ): (url, txt_name) for (url, txt_name) in all_urls  # 遍历元组
            }

            for _ in as_completed(futures):
                pbar.update(1)

    # 输出统计信息
    print(f"\n下载完成！成功下载 {counter.value}/{len(all_urls)} 个文件")
    print(f"保存目录：{os.path.abspath(target_folder)}")

    # 输出错误日志
    if not error_queue.empty():
        print("\n错误日志（最多显示20条）：")
        for _ in range(min(20, error_queue.qsize())):
            print(f"• {error_queue.get()}")

input_folder = "/content/drive/MyDrive/513/A50_pdf_download_links/"
output_folder = "/content/drive/MyDrive/513/A50_pdfs_named/"

download_pdfs_from_txts(input_folder, output_folder)